---
title: Configuring Autograders
---

## Summary of Autograding Architecture

The solutions repository automatically contains a bare-bones `pawtograder.yml` file. This file configures the autograder for the assignment using the "overlay grader". The "overlay grader" is broken into three separate units: the `build` configuration, the collection of `gradedParts`, and the `submissionFiles` that make up a student submission.

In brief, the following occurs when the autograder is invoked

1. The system checks for all the files under the `submissionFiles` section. If any of those files are missing, the autograder aborts and reports those files are missing in the Grading Summary.
2. The autograder runs according to the `build` configuration. If the run fails, the failure is reported and no autograder results are given.
3. A grading summary is generated from the results of the tests run. This summary is organized according the the `gradedParts` collection and reported to the students in their submission.
s4. The `submissionFiles` are listed in the "Files" section of the student's results on Pawtograder.
5. The environment is cleaned up for the next invocation of the autograder.

What follows is a description of the various properties of the "overlay grader" in the `pawtograder.yml` file.

# Build configuration

The `build` property describes what the autograder will do for each submission. Below are all of the sub-properties that can be set to configure the autograder itself. However, only `preset` is required.

- `preset`: choose one of the available build presets. Currently, there are three presets:
  - `java-gradle` for building Java projects that use Gradle
  - `python-script` for building Python projects with custom scripts
  - `none` for custom project setups
- `cmd`:the CLI command for running your testing infrastructure.
- `artifacts`: add an array of artifacts to be generated by the build. //TODO: Is this true?
- `linter`: specify how code style should be checked and the results of running that liner. This property contains two required sub-properties
  - `preset`: the linter you would like to use. Right now, "checkstyle" is available as the only preset value.
  - `policy`: what should be done if a linter error is found. There are two values for this property: 'ignore' or 'fail'. In either case,  the linter errors will be reported in the Grading Summary on Pawtograder.
    - 'ignore' allows the student submission to be run on the autograder and display those results.
    - 'fail' _prevents_ the student code from being run on autograder results. Using this value means any linter error will result in a zero from the autograder and no autograder results will be shown.
- `student_tests`: determines what to do with the student tests on both the student implementation under the `student_impl` sub-property and the instructor implementation under the `instructor_impl` sub-property. The `student_impl` property has a special sub-property, `report_branch_coverage` which reports the coverage of the student tests on their own implementation. Otherwise, both sub-properties contain the following optional properties:
  - `run_tests`: a boolean flag for whether student tests should be run on the implementation in question.
  - `run_mutation`: a boolean flag which runs the student tests under mutations of the implementation in question.
  - `report_mutation_coverage` which generates a student-visible report of mutation testing by mutating the implementation in question.
- `timeouts_seconds` for setting timeouts for a few parts of the autograder system. The timeout for each part is specified as the following optional sub-properties.
  - `build`: timeout in seconds for the build process
  - `instructor_tests`: timeout in seconds for each instructor test
  - `student_tests`: timeout in seconds for each student test
  - `mutants`: timeout in seconds for each run in mutation testing
- `venv` for describing the virtual environment for your scripts. Both sub-properties are required
  - `dir_name`: the name of the directory that houses the virtual environment
  - `cache_key`: a key to fetch the cached version of this virtual environment //TODO: No idea what this is.
- `script_info` for defining the necessary commands to setup and run the script. All of the following sub-properties are required.
  - `install_deps`: The command to install any requirements for your script.
  - `setup_venv`: The command to set up the virtual environment for the script.
  - `activate_venv`: The command to activate the virtual environment for the script.
  - `linting_report`: The command to generate the reports for your linter.
  - `html_coverage_reports`: The command to generate the HTML version of your test coverage reports.
  - `textual_coverage_reports`: The command to generate the text version of your coverage reports.
  - `test_runner`: The command to run the driver of your test suite.
  - `mutation_test_runner`: The command to run the driver of your mutation test suite.

# Graded parts

The `gradedParts` property describes what tests will be reported and how many points they are worth in the assignment. The value of this property is an array of items. Each item is made of two required sub-properties.

- `name`: the name of the graded part
- `gradedUnits`: an array of the specific units that make up a graded parts

There is also an optional property for displaying the results in a submission.

- `hide_until_released`: a boolean for whether this part's scores and tests are visible to students only when released. By default, this is false, meaning the graded part and its units are visible throughout the assignment lifetime.

The `gradedUnits` are made of items called _units_. Each unit can either be regular graded units like JUnit tests or they can be mutation test units. These unit types can both be used in the same graded part if desired.

- Regular graded units are composed of 5 properties:
  - `name`: a name for the graded unit
  - `points`: the number of points allocated to that unit
  - `testCount`: the number of tests in that unit
  - `tests`: an array of tests. Each test is either a fully quantified method name or a fully quantified class name. If it is the latter, then all tests in that class are run and are counted toward the number of tests. In total, the number of tests must match `testCount`.
  - `allow_partial_credit`: an optional boolean flag to allow for partial credit. Partial credit is calculated as `number of tests passed` / `testCount`. If the flag is set to false, then the student implementation must pass all tests to get any credit for the grading unit. By default, partial credit is enabled. //TODO: Is it enabled by default?

- Mutation testing units are composed of the following and all are required
  - `name`: a name for the graded unit
  - `locations`: an array of locations. A location is an array of strings reporting the locations of the mutants //TODO: Is this true?
  - `breakpoints`: an array of breakpoints. Each breakpoint has two required sub-properties
    - minimumMutantsDetected: the minimum number of mutants some test should find
    - pointsToAward: the number of points awarded for finding the minimum number of mutants

# Submission files

The `submissionFiles` property describes the files that are accepted by Pawtograder for manual grading and testing. These are split into two necessary sub-fields: `files` and `testFiles`.

- `file`_ are the source code and any auxiliary files you want to either automatically grade or manually grade.
- `test_files` are the locations of test files you want to either autograder, report about, or manually grade.

Both are declared as arrays of file path expressions describing a set of zero or more files. You can use globs much like in `grep` and `find` to collect sub-directories or files with specific extensions.

These files **must** be UTF-8 text files. If a submission file is a binary executable, image, or some other non-text file, Pawtograder will not run the autograder and instead report an error. This error is presented in the Grading Summary and will request the files be text files. Furthermore, no files will be added to the submission as well.

## Github Workflow and `grade.yml`

The template repository, which is cloned into student repositories, contains a GitHub workflow file in `.github/workflows/grade.yml`. Before the assignment is released to students, you will need to edit that `grade.yml` file to setup the grading task properly. At bare minimum, the steps section must be filled out to install whatever dependencies are necessary for the autograder.

Both you and students can see each job executed in the Actions section of GitHub for more detailed information in the event of a failure.

When a student pushes their code to Github on the branches specified in `grade.yml`, the grading job activates and proceeds through each of the steps. By default, there are three steps performed

1. "Checkout", which checks out the student's code onto the image.
2. The steps for installing dependencies. You should have either uncommented the default dependency steps or added your own.
3. "Collect Submission and Run Grader" which runs the submission according to the autograder configuration in `pawotgrader.yml` in the solution repository.

Pawtograder automatically populates steps 1 and 3 automatically. Step 2 requires manual editing, but common installation step actions are generated in the `grade.yml` of the template repository. Thee steps are commented out to prevent issues with building, but they can be uncommented if they prove useful. If you wish to add your own actions, you can learn more at the [GitHub actions documentation site](https://docs.github.com/en/actions).

## Quickstart: Java and Python
- TODO: Describe build system integration

This section contains some example `pawtograder.yml` files used for real Java and Python assignments. Feel free to use these as templates for configuring your own autograders.

# Basic Java project with JUnit tests and Gradle build system

Below is an example `grade.yml` and `pawtograder.yml` for a Java project using JUnit tests. This Java project uses Gradle as its build system, which is important to know for the `pawtograder.yml` file. Otherwise, the `build.gradle` file is the default generated by Pawtograder in the template and solution repositories.

This `pawtograder.yml` file builds and runs the project using the default Gradle `test` task. Of course, this task must be properly defined in the `build.gradle` included with the student and solution repositories to run. The graded parts are JUnit tests that are part of the solution directory. Notice that this version opted to list all of the tests, requiring fully quantified method names.
```yaml
# yaml-language-server: $schema=https://raw.githubusercontent.com/pawtograder/assignment-action/refs/tags/v3/pawtograder.schema.json

grader: 'overlay'
build:
  preset: 'java-gradle'
  cmd: './gradlew test'
  linter:
    preset: 'checkstyle'
    policy: 'ignore'
  student_tests:
    student_impl:
      report_branch_coverage: true
      report_mutation_coverage: false
gradedParts:
  - name: Public Tests
    gradedUnits:
      - name: Valid Construction
        points: 1
        testCount: 1
        allow_partial_credit: true
        tests:
          - CreditCardPublicTest.testValidConstruction
      - name : Invalid Construction
        points: 3
        testCount: 3
        allow_partial_credit: true
        tests:
          - CreditCardPublicTest.testInvalidCreditLimitOnConstruction
          - CreditCardPublicTest.testInvalidAprOnConstruction
          - CreditCardPublicTest.testInvalidLateFeeOnConstruction
submissionFiles:
  files:
    # The default is to submit all java, arr, md, and python files.
    # You should almost definitely change this.
    - 'src/main/java/**/*.java'
  testFiles:
    - 'src/test/java/**/*.java'
```

This is the `grade.yml` for that same assignment. These are the GitHub workflow that runs with each student submission It is the bare minimum version that installs Java 21 Temurin on an image and runs the above autograder whenever we push to the `main` branch. 

```yaml
name: Submit Assignment and Run Grader
permissions:
  id-token: write
  contents: read
on:
  workflow_dispatch:

  push:
    branches:
      - main

jobs:
  grade:
    name: Submit and Grade Assignment
    runs-on: khoury-course-runners
    steps:
      - name: Checkout
        id: checkout
        uses: actions/checkout@v4
        with:
          path: submission
      - name: Install Java
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '21'
      - name: Collect Submission and Run Grader
        uses: pawtograder/assignment-action@v3
        with:
          grading_server: 'https://api.pawtograder.com'
          action_ref: '${{ github.action_ref }}'
          action_repository: '${{ github.action_repository }}'
```

# Python testing with custom scripts

Here is an example `pawtograder.yml` for a Python project. In this version, style was relegated to a separate test in the `gradedParts` property. Therefore, the linter was set to `ignore` since students always needed to see the autograder results.

The `yaml-language-server` line is also different between the examples. This version always points to a local schema where as the basic Java project example points to a specific version of this schema external to Pawtograder.

```yaml
# yaml-language-server: $schema=./pawtograder.schema.yml
build:
  preset: 'python-script'
  linter:
    preset: 'checkstyle'
    policy: 'ignore'
  student_tests: 
    student_impl: 
      run_tests: true
      report_branch_coverage: false
    instructor_impl: 
      run_tests: true
      run_mutation: false 
      report_mutation_coverage: false
  venv:
    dir_name: '.venv'
    cache_key: 'sp26-cs2100-lab0'
  script_info: 
    install_deps: 'pip install -r requirements.txt'
    setup_venv: 'python3 -m venv .venv'
    activate_venv: '. .venv/bin/activate'
    linting_report: './generate_linting_reports.sh'
    html_coverage_reports: './generate_coverage_reports.sh'
    textual_coverage_reports: './generate_textual_coverage_reports.sh'
    test_runner: 'python3 test_runner.py'
    mutation_test_runner: 'python3 mutation_test_runner.py'


grader: 'overlay'

gradedParts:
  - name: Instructor tests on Student Implementation
    gradedUnits:
      - name: Is a palindrome
        tests:
          - test_q1.TestIsPalindromeTrue
        points: 20
        testCount: 1
      
      - name: Is not a palindrome
        tests:
          - test_q1.TestIsPalindromeFalse
        points: 20
        testCount: 1
      
      - name: Empty string is a palindrome
        tests:
          - test_q1.TestIsPalindromeEmptyString
        points: 20
`        testCount: 1
      
      - name: Style (Pylint and Mypy)
        tests:
          - test_style.TestStyleReports
        points: 40
        testCount: 1

submissionFiles:
  files:
    - 'src/*.py'
  testFiles:
    - 'tests/test_*.py'
```

# Advanced Java autograder with mutation testing

This section covers an example Java assignment with mutation testing. This project uses Gradle as the build system to not only allow for general testing, but also to incorporate the libraries needed to do mutation testing as well as other custom plugins. Readers of this section can use the below as a way to model Java projects that need more plugins than the standard template Pawtograder provides.

Below is a `pawtograder.yml` for this Java project. You will notice that the student tests section has the `run_mutation` property set to true.
To save time on the instructor side, this example uses the fully quantified class name and runs all of the tests within.

```yaml
# yaml-language-server: $schema=https://raw.githubusercontent.com/pawtograder/assignment-action/refs/tags/v3/pawtograder.schema.json
grader: 'overlay'
build:
  preset: 'java-gradle'
  cmd: './gradlew test'
  linter:
    preset: 'checkstyle'
    policy: 'fail'
  student_tests:
    instructor_impl:
      run_tests: true
    student_impl:
      run_tests: true
      run_mutation: true
      report_mutation_coverage: true
      report_branch_coverage: true

gradedParts:
  - name: Student-Visible Test Results
    gradedUnits:
      - name: Simple BoxSet Visible
        points: 35
        testCount: 7
        allow_partial_credit: true
        tests:
          - SimpleBoxSetVisibleTest.
  - name: Hidden Test Results
    hide_until_released: true
    gradedUnits:
      - name: Simple BoxSet Hidden
        points: 25
        testCount: 5
        allow_partial_credit: true
        tests:
          - SimpleBoxSetHiddenTest.
submissionFiles:
  files:
    - 'src/main/java/box/BoxSet.java' # This file is REQUIRED, if missing a submission will not be created
    - 'src/main/java/box/SimpleBoxSet.java' # This file is REQUIRED, if missing a submission will not be created
    - 'src/main/java/**/*.java' # All files that match this glob pattern will be submitted. There are no restrictions on how many files will match.
  testFiles:
    - 'src/test/java/SimpleBoxSetTest.java' # This file is REQUIRED, if missing a submission will not be created
    - 'src/test/java/**/*.java' # All files that match this glob pattern will be submitted. There are no restrictions on how many files will match.
```

Next we look at the `build.gradle` file. Unlike the prior Java example, we need to edit the build.gradle file to incorporate the mutation testing library. In brief

1. The `plugins` section need to know the name of the mutation testing library, pitest, and its version.
2. A `pitest` section is added to configure the mutation testing. You can find more on how to configure this on the [README for the Gradle plugin for PiTest](https://github.com/szpak/gradle-pitest-plugin).

```
plugins {
    id 'java'
    id 'application'
    id 'checkstyle'
    id 'jacoco'
    id 'info.solidsoft.pitest' version '1.15.0'
}

group 'boxset'
version '1.0-SNAPSHOT'

repositories {
    mavenCentral()
}

sourceSets {
    test {
        java {
            srcDirs = ['src/test/java']
        }
    }
}

dependencies {
    testImplementation 'junit:junit:4.13.2'
    testImplementation 'org.testng:testng:7.1.0'
}

jacocoTestReport {
    dependsOn test
    reports {
        html.required = true
        xml.required = false
        csv.required = true
    }
}

test {
    useJUnit()
    finalizedBy tasks.jacocoTestReport
    ignoreFailures = true
}

application {
    mainClass = 'box.SimpleBoxSet'
}

checkstyle {
    toolVersion = '10.23.1'
    configFile = file("${rootDir}/config/checkstyle/checkstyle.xml")
    maxWarnings = 0
    ignoreFailures = false
}

tasks.withType(Checkstyle) {
    reports {
        xml.required = true
        html.required = true
    }
}

// PIT Mutation Testing Configuration
pitest {
    targetClasses = ['box.*'] // Adjust to match your package structure
    targetTests = ['*'] // Adjust to match your test package structure
    pitestVersion = '1.15.8'
    threads = 4
    outputFormats = ['XML', 'HTML']
    timestampedReports = false
//    mutationThreshold = 90
    // Exclude certain mutators if needed
    // excludedMethods = ['hashCode', 'equals']
    // Include specific mutators (optional - by default all are included)
    // mutators = ['STRONGER']
    // Test plugin configuration for JUnit 4
    testPlugin = 'junit'
    // Verbose output
    verbose = true
    // Export line coverage data
    exportLineCoverage = true
    // Fail build if mutation score is below threshold
    failWhenNoMutations = false
}
```